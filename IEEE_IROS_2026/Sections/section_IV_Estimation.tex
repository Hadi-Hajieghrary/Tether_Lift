% !TEX root = ../Main.tex
\begin{figure}[t!]
  \centering
  \includegraphics[width=0.84\columnwidth]{Figures/fig_mass_convergence.png}
  \caption{Adaptive mass estimation convergence. \textit{Top:} Per-quadrotor concurrent learning estimates $\hat{\theta}_i \to m_L/N = 1.0$\,kg, converging within $\sim$8\,s. \textit{Bottom:} Comparison with gradient-only adaptation, which requires $\sim$22\,s and exhibits larger steady-state oscillation.}
  \label{fig:mass_convergence}
\end{figure}

Each quadrotor runs three estimators in a downward cascade---no payload, cable, or adaptive-parameter states are exchanged: (i)~ESKF for navigation, (ii)~geometric load-state filter, and (iii)~concurrent learning mass estimator. Timescale separation---navigation at 200\,Hz versus load/mass estimation at 50\,Hz---lets each layer treat inputs as quasi-static, with upstream errors propagating unidirectionally to simplify stability analysis.

\subsection{Navigation: Error-State Kalman Filter}

A 15-state ESKF~\cite{sola2017quaternion} with $\delta x = [\delta p,\,\delta v,\,\delta\theta,\,\delta b_a,\,\delta b_g]^\top \in \R^{15}$ fuses IMU (200\,Hz), GPS (10\,Hz), and barometer (25\,Hz). Propagation uses bias-corrected specific force $a_W = R(\bar{q})(\tilde{a} - b_a) - ge_3$ and angular velocity $\omega_c = \tilde{\omega} - b_g$. Covariance propagation uses the error-state Jacobian: $P \leftarrow \Phi P\Phi^\top + Q_d$. GPS and barometer corrections use the Joseph-form covariance formula, with the barometer providing altitude observability during GPS outages. The ESKF provides $(\hat{p}_i, \hat{v}_i, \hat{R}_i)$ to all downstream layers; control closes through the estimator, not ground truth.

\subsection{Decentralized Load-State Estimation}

When cable~$i$ is taut, payload position is estimated geometrically: $z_{p_i} = \hat{p}_i - L_i\,n_i$, where $n_i \in \Sph^2$ points from quadrotor to payload and $L_i$ is cable rest length. Each quadrotor maintains a Kalman filter with state $[\hat{p}_{L,i}^\top, \hat{v}_{L,i}^\top]^\top \in \R^6$. The prediction uses a constant-velocity model with damping ($\beta_v = 0.05$), biasing the payload velocity toward the quadrotor velocity, reflecting the quasi-static assumption during slow transport. Measurement noise is modulated by tension confidence:
\begin{equation}
  \xi_T = \min\!\left(1,\,T_i/T_{\text{conf}}\right), \quad R_k \leftarrow R_k / (0.1 + 0.9\,\xi_T),
  \label{eq:tconf}
\end{equation}
with $T_{\text{conf}} = 20$\,N. When the cable is slack, measurement variance inflates and the filter relies on prediction. An outlier gate ($\kappa_\nu = 3.0$) prevents cable whip from corrupting estimates.

With a single cable, payload position is observable only along the cable direction; tangential components depend on prediction. Section~\ref{sec:results} quantifies this gap: the decentralized estimator is roughly $4\times$ worse than a centralized baseline that fuses all $N$ cable constraints but requires inter-agent communication.

From the per-cable vertical force equilibrium of~\eqref{eq:payload}, each quadrotor constructs a scalar parametric model:
\begin{equation}
  \underbrace{T_i \cos\phi_i}_{\varphi_i} = \underbrace{\norm{g\,e_3 + \hat{a}_L}}_{Y_i} \cdot \underbrace{\frac{m_L}{N}}_{\theta} + \varepsilon_i,
  \label{eq:regressor}
\end{equation}
where $\phi_i = \arccos(-n_{i,z})$ is cable angle from vertical, $\hat{a}_L$ is payload acceleration (via numerical differentiation with low-pass filter, $\tau_f = 0.1$\,s), and $\varepsilon_i$ captures modeling error from non-equilibrium dynamics, cable sag, and length asymmetry. Unequal cable lengths cause per-cable vertical force $T_i\cos\phi_i$ to deviate from $m_Lg/N$; this enters $\varepsilon_i$ and increases the ultimately bounded error. In simulation, 19\% cable asymmetry yields steady-state error below 0.1\,kg per agent.

\textit{Implicit coordination.}
The regressor~\eqref{eq:regressor} formalizes the implicit coordination stated in~\eqref{eq:force_convergence}. With feedforward $F_{\text{ff},i} = \hat{\theta}_i \cdot u$ where $u := g\,e_3 + \ddot{p}_L^d$, as $\hat{\theta}_i \to m_L/N$ per agent the total feedforward satisfies
\begin{equation}
  \textstyle\sum_{i=1}^{N} F_{\text{ff},i} = \sum_{i=1}^{N} \hat{\theta}_i\, u \;\longrightarrow\; m_L\, u.
  \label{eq:implicit_coord}
\end{equation}
No agent requires knowledge of $N$ or $m_L$; each estimates its share from local cable measurements, and summation yields correct collective compensation. This holds because the regressor $Y_i$ and control $u$ derive from the same shared trajectory, ensuring scalar parametric consistency across agents.

\begin{remark}[Trajectory synchronization]\label{rem:sync}
Since the trajectory is a polynomial stored locally and evaluated analytically, agents compute $u$ identically up to floating-point precision ($\sim\!10^{-15}$ relative error). Any clock offset $\Delta t$ introduces feedforward error bounded by $\norm{\partial u/\partial t}\Delta t$, which is $O(\Delta t)$\,m/s$^2$ at the tested velocities.
\end{remark}

The concurrent learning update augments gradient descent with stored data is
$\dot{\hat{\theta}}_i = -\gamma\,Y_i\,s_{\text{proj}} - \gamma\rho \sum_{j=1}^{M_i} Y_j(Y_j\hat{\theta}_i - \varphi_j)$, where $\gamma = 0.5$, $\rho = 0.5$, and $s_{\text{proj}} = s_i^\top n_i$ projects sliding variable $s_i = \dot{e}_{L,i} + \lambda e_{L,i}$ ($\lambda = 1.0$) onto cable direction~\cite{chowdhary2010concurrent}. The first term drives online gradient descent; the second replays stored pairs $\{(Y_j, \varphi_j)\}_{j=1}^{M_i}$. The history buffer holds up to $\bar{M} = 50$ points, admitting samples only when excitation exceeds $Y_{\min} = 0.5$\,m/s$^2$ and the regressor differs from the buffer mean by $\delta_Y = 0.1$, ensuring diversity. Estimates are projected to $[\theta_{\min}, \theta_{\max}] = [0.1, 50.0]$\,kg.

\begin{proposition}[Convergence without PE]\label{prop:cl}
If $\Sigma_Y = \frac{1}{M}\sum_j Y_j^2 > 0$ (at least one informative sample), the parameter error satisfies an exponential bound; $\abs{\tilde{\theta}(t)} \leq \abs{\tilde{\theta}(0)}\exp(-\gamma\rho\,\Sigma_Y\,t)$, independently of online excitation. With bounded modeling error $|\varepsilon_i| \leq \bar{\varepsilon}$, convergence is uniformly ultimately bounded: $|\tilde{\theta}| \leq \bar{\varepsilon}/(\rho\,\Sigma_Y)$.
\end{proposition}

\begin{proof}
With $V_\theta = \tilde{\theta}^2/(2\gamma)$, the time derivative has two contributions. The online gradient term contributes $-\tilde{\theta}\,Y_i\,s_{\text{proj}}$; under projection onto $[\theta_{\min}, \theta_{\max}]$, $-\tilde{\theta}\,\text{Proj}(\cdot) \leq 0$~\cite{chowdhary2013exponentially}, so this term only strengthens the bound and its omission is conservative. The concurrent learning term gives $\dot{V}_\theta \leq -\gamma\rho\Sigma_Y\tilde{\theta}^2$; Gr\"{o}nwall's inequality yields exponential bound. Bounded $|\varepsilon_i| \leq \bar{\varepsilon}$ introduces a residual limiting convergence to the UUB ball. The convergence rate $\gamma\rho\Sigma_Y$ depends on trajectory informativeness; empirically, $\sim$30 informative samples accumulated during ascent yield settling in $\sim$8\,s.
\end{proof}
